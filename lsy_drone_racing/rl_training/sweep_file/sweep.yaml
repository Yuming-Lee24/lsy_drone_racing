program: ppo_racing.py
method: bayes  # 使用贝叶斯优化（比随机搜索更聪明，能更快找到最优解）
metric:
  name: train/episode_reward  # 优化目标：最大化奖励
  goal: maximize

parameters:
  # --- 1. 训练控制 ---
  train:
    value: True
  wandb_enabled:
    value: True
  total_timesteps:
    value: 1000000  # 每个实验只跑 100万步 (足够看能不能飞了)
  
  # --- 2. PPO 核心参数 ---
  learning_rate:
    distribution: log_uniform_values
    min: 1e-5
    max: 5e-4
  
  ent_coef:
    # 关键！现在的 0.0 可能太低，导致它太早收敛到“落地”策略
    # 我们尝试给它多一点探索欲
    distribution: uniform
    min: 0.0
    max: 0.05 

  num_steps:
    # 视野长度。太短的话，它看不到“坚持飞一会儿就能过门”的好处
    values: [128, 256, 512]

  # --- 3. 奖励系数 (重中之重) ---
  # 既然它容易掉下来，我们调整一下它的“价值观”
  
  coef_progress:
    # 前进奖励：给大一点，诱惑它往前冲
    distribution: uniform
    min: 5.0
    max: 20.0
    
  coef_collision:
    # 撞击惩罚：如果惩罚太重，它就不敢动；太轻，它就乱撞
    distribution: uniform
    min: 0.1
    max: 5.0

command:
  - ${env}
  - python
  - ${program}
  - ${args}