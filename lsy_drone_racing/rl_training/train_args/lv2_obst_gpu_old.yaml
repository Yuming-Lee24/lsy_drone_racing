# ========== 训练规模参数 (保持不变) ==========
num_envs: 1024
num_steps: 96
num_minibatches: 8
total_timesteps: 200000000
seed: 123

# ========== PPO核心参数 ==========
learning_rate: 0.0003               # ↓ 从0.000489降低，更谨慎的更新
anneal_lr: True                     # 保持
gamma: 0.98                         # 保持
gae_lambda: 0.95                    # 保持
update_epochs: 10                   # ↓ 从12降低，避免过拟合短episode
norm_adv: True                      # 保持
clip_coef: 0.25                     # ↑ 从0.2提高，允许policy更大调整
clip_vloss: True                    # 保持
ent_coef: 0.025                     # ↑↑ 从0.0072大幅提高，关键改动！
vf_coef: 0.6                        # 保持
max_grad_norm: 0.4                  # 保持

# ========== 网络架构 ==========
hidden_dim: 256                     # 保持
n_history: 2                        # 保持

# ========== 奖励函数权重 ==========
coef_progress: 15                   # ↓ 从20降低，减少激进性
coef_gate: 10                       # 保持
coef_finish: 100                    # 保持
coef_collision: 25                  # ↑ 从10提高，强化安全信号
coef_smooth: 0.2                    # ↑ 从0.1提高，要求更精细控制
coef_spin: 0.15                     # ↑ 从0.1微调，减少不必要旋转
coef_align: 0.5                     # 保持
coef_time: 0.05                     # 保持

# ========== 环境配置 ==========
config_file: level2.toml    # 根据你的track调整
jax_device: gpu
cuda: True
torch_deterministic: True

# ========== Wandb配置 ==========
wandb_enabled: True
wandb_project_name: DroneRacing-PPO
wandb_entity: null